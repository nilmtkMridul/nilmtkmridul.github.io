<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NILMTK Blog Update</title><link>/</link><description></description><atom:link href="/feeds/mridul-malpotra.rss.xml" rel="self"></atom:link><lastBuildDate>Mon, 20 Oct 2014 11:21:00 +0200</lastBuildDate><item><title>Problems with the ECO Converter</title><link>/problems-eco-converter.html</link><description>&lt;p&gt;ECO Converter is being made at &lt;a href="https://github.com/mridulmalpotra/nilmtk/blob/master/nilmtk/dataset_converters/ECO"&gt;this repository.&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Few problems as of now in the converter&lt;/h1&gt;
&lt;h2&gt;Problem 1: Memory Error&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Memory Error" src="../images/mem_error.png" /&gt;&lt;/p&gt;
&lt;p&gt;While implementing the dataset, the primary problem that I am facing is the large size of the dataset. The zipped files of the Smart meter readings were about 2.5 GB in size. This decompressed and added with a timestamp of &lt;em&gt;numpy.datetime64&lt;/em&gt; makes the file size large.&lt;/p&gt;
&lt;h3&gt;Dataset structure&lt;/h3&gt;
&lt;p&gt;ECO Dataset has folder 'i_sm_csv' and 'i_plug_csv' where i is the building no.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;i_sm_csv has a folder i&lt;/li&gt;
&lt;li&gt;i_plug_csv has a folder 01, 02,....n where n are the plug numbers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each folder has a CSV file per day, with each file containing 86400 entries mapping every second of that day. Even for plug data which has a compressed zip size totalling 120 MB, owing to the 64-bit time stamp we get the hdf5file having a size &lt;em&gt;5-6 GB&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;The storage is even greater for a smart meter, making the final file large in size.&lt;/p&gt;
&lt;h3&gt;Low resources&lt;/h3&gt;
&lt;p&gt;My initial location for the HDF5file was a drive with probably not enough storage space as it encountered memory error quite easily. Doing the same in another disk partition with plenty space made the program last longer, but it freezes my system before completing even 1 smart_meter_csv. The CPU was becoming a throttle here as a single core is used 100%. Possible multithreading implementations could help, but cannot say with surety.&lt;/p&gt;
&lt;h2&gt;Problem 2: Slow Algorithm&lt;/h2&gt;
&lt;p&gt;My approach to calculating the dataframe for a meter is to initialize a dataframe and keep appending a dataframe per csv file. This means around 200-250 operations per meter per folder. This approach is linear and takes a long time for large data. As we are simply appending the dataframes together and sorting by index in the end, one clear method to speed up the process is using a divide and conquer approach and reduce the overall complexity to O(log(n)). This puts a lesser load on the CPU. If this will benefit the algorithm, I will implement it in the converter.&lt;/p&gt;
&lt;p&gt;However, the dataframe object created per meter becomes too huge for the smart meter computations. I will try to find some alternative for storing in the hdf5file.&lt;/p&gt;
&lt;h4&gt;&lt;em&gt;Mridul&lt;/em&gt;&lt;/h4&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mridul Malpotra</dc:creator><pubDate>Mon, 20 Oct 2014 11:21:00 +0200</pubDate><guid>tag:,2014-10-20:problems-eco-converter.html</guid><category>eco</category><category>converter</category><category>nilmtk</category><category>hdf5</category><category>memory</category><category>metadata</category></item></channel></rss>