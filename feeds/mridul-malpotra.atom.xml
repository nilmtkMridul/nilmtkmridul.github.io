<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>NILMTK Blog Update</title><link href="/" rel="alternate"></link><link href="/feeds/mridul-malpotra.atom.xml" rel="self"></link><id>/</id><updated>2014-10-29T01:27:00+01:00</updated><entry><title>ECO Converter - How it fared</title><link href="/eco-converter-fared.html" rel="alternate"></link><updated>2014-10-29T01:27:00+01:00</updated><author><name>Mridul Malpotra</name></author><id>tag:,2014-10-29:eco-converter-fared.html</id><summary type="html">&lt;p&gt;ECO Converter is being made at &lt;a href="https://github.com/mridulmalpotra/nilmtk/blob/master/nilmtk/dataset_converters/ECO"&gt;this repository.&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Few problems that were solved&lt;/h1&gt;
&lt;h3&gt;Problems: Memory Error and Slow Algorithm&lt;/h3&gt;
&lt;p&gt;As mentioned earlier, too much memory and too less resources were allocated. Some approaches to solving this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;As suggested by Nipun sir, I implemented an appending function to the existing HDF5file. Hence in this scenario, we would not merge the concatinated dataframe all at once but append it for every file. This made the speed of the program predictable, error-free and executed perfectly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This was also the time when I upgraded my laptop to a new one. Although I had to install and run nilmtk and its all dependencies from scratch, it made execution faster and I had more RAM and CPU speed to allot. This also helped in the process.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;em&gt;Mridul&lt;/em&gt;&lt;/h4&gt;</summary><category term="eco"></category><category term="converter"></category><category term="nilmtk"></category><category term="hdf5"></category><category term="memory"></category><category term="metadata"></category></entry><entry><title>Problems with the ECO Converter</title><link href="/problems-eco-converter.html" rel="alternate"></link><updated>2014-10-20T11:21:00+02:00</updated><author><name>Mridul Malpotra</name></author><id>tag:,2014-10-20:problems-eco-converter.html</id><summary type="html">&lt;p&gt;ECO Converter is being made at &lt;a href="https://github.com/mridulmalpotra/nilmtk/blob/master/nilmtk/dataset_converters/ECO"&gt;this repository.&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Few problems as of now in the converter&lt;/h1&gt;
&lt;h2&gt;Problem 1: Memory Error&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Memory Error" src="../images/mem_error.png" /&gt;&lt;/p&gt;
&lt;p&gt;While implementing the dataset, the primary problem that I am facing is the large size of the dataset. The zipped files of the Smart meter readings were about 2.5 GB in size. This decompressed and added with a timestamp of &lt;em&gt;numpy.datetime64&lt;/em&gt; makes the file size large.&lt;/p&gt;
&lt;h3&gt;Dataset structure&lt;/h3&gt;
&lt;p&gt;ECO Dataset has folder 'i_sm_csv' and 'i_plug_csv' where i is the building no.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;i_sm_csv has a folder i&lt;/li&gt;
&lt;li&gt;i_plug_csv has a folder 01, 02,....n where n are the plug numbers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each folder has a CSV file per day, with each file containing 86400 entries mapping every second of that day. Even for plug data which has a compressed zip size totalling 120 MB, owing to the 64-bit time stamp we get the hdf5file having a size &lt;em&gt;5-6 GB&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;The storage is even greater for a smart meter, making the final file large in size.&lt;/p&gt;
&lt;h3&gt;Low resources&lt;/h3&gt;
&lt;p&gt;My initial location for the HDF5file was a drive with probably not enough storage space as it encountered memory error quite easily. Doing the same in another disk partition with plenty space made the program last longer, but it freezes my system before completing even 1 smart_meter_csv. The CPU was becoming a throttle here as a single core is used 100%. Possible multithreading implementations could help, but cannot say with surety.&lt;/p&gt;
&lt;h2&gt;Problem 2: Slow Algorithm&lt;/h2&gt;
&lt;p&gt;My approach to calculating the dataframe for a meter is to initialize a dataframe and keep appending a dataframe per csv file. This means around 200-250 operations per meter per folder. This approach is linear and takes a long time for large data. As we are simply appending the dataframes together and sorting by index in the end, one clear method to speed up the process is using a divide and conquer approach and reduce the overall complexity to O(log(n)). This puts a lesser load on the CPU. If this will benefit the algorithm, I will implement it in the converter.&lt;/p&gt;
&lt;p&gt;However, the dataframe object created per meter becomes too huge for the smart meter computations. I will try to find some alternative for storing in the hdf5file.&lt;/p&gt;
&lt;h4&gt;&lt;em&gt;Mridul&lt;/em&gt;&lt;/h4&gt;</summary><category term="eco"></category><category term="converter"></category><category term="nilmtk"></category><category term="hdf5"></category><category term="memory"></category><category term="metadata"></category></entry></feed>